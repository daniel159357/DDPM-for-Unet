{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZ3QmI-mw7KP"
      },
      "outputs": [],
      "source": [
        "# https://hackmd.io/@Tu32/Bkq3fQi0s # unet pytorch code (Chinese)\n",
        "# https://nn.labml.ai/diffusion/ddpm/unet.html # unet pytorch code (ENG)\n",
        "# https://blog.csdn.net/dgvv4/article/details/128335187 # for shufflenetv2 pytorch code\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ChannelShuffle(nn.Module):\n",
        "    def __init__(self,groups):\n",
        "        super().__init__()\n",
        "        self.groups=groups\n",
        "    def forward(self,x):\n",
        "        n,c,h,w=x.shape\n",
        "        x=x.view(n,self.groups,c//self.groups,h,w) # group\n",
        "        x=x.transpose(1,2).contiguous().view(n,-1,h,w) #shuffle\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "KC4ZIhvZxeNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBnSiLu(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0):\n",
        "        super().__init__()\n",
        "        self.module=nn.Sequential(nn.Conv2d(in_channels,out_channels,kernel_size,stride=stride,padding=padding),\n",
        "                                  nn.BatchNorm2d(out_channels),\n",
        "                                  nn.SiLU(inplace=True))\n",
        "    def forward(self,x):\n",
        "        return self.module(x)"
      ],
      "metadata": {
        "id": "o_gyO5ajxeLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBottleneck(nn.Module):\n",
        "    '''\n",
        "    shufflenet_v2 basic unit(https://arxiv.org/pdf/1807.11164.pdf)\n",
        "    '''\n",
        "    def __init__(self,in_channels,out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.branch1=nn.Sequential(nn.Conv2d(in_channels//2,in_channels//2,3,1,1,groups=in_channels//2), #https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
        "                                    nn.BatchNorm2d(in_channels//2),\n",
        "                                    ConvBnSiLu(in_channels//2,out_channels//2,1,1,0))\n",
        "        self.branch2=nn.Sequential(ConvBnSiLu(in_channels//2,in_channels//2,1,1,0),\n",
        "                                    nn.Conv2d(in_channels//2,in_channels//2,3,1,1,groups=in_channels//2),\n",
        "                                    nn.BatchNorm2d(in_channels//2),\n",
        "                                    ConvBnSiLu(in_channels//2,out_channels//2,1,1,0))\n",
        "        self.channel_shuffle=ChannelShuffle(2)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x1,x2=x.chunk(2,dim=1) #https://pytorch.org/docs/stable/generated/torch.chunk.html#torch.chunk\n",
        "        x=torch.cat([self.branch1(x1),self.branch2(x2)],dim=1)\n",
        "        x=self.channel_shuffle(x) #shuffle two branches\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "WgEOhu1GxeI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualDownsample(nn.Module):\n",
        "    '''\n",
        "    shufflenet_v2 unit for spatial down sampling(https://arxiv.org/pdf/1807.11164.pdf)\n",
        "    '''\n",
        "    def __init__(self,in_channels,out_channels):\n",
        "        super().__init__()\n",
        "        self.branch1=nn.Sequential(nn.Conv2d(in_channels,in_channels,3,2,1,groups=in_channels),\n",
        "                                    nn.BatchNorm2d(in_channels),\n",
        "                                    ConvBnSiLu(in_channels,out_channels//2,1,1,0))\n",
        "        self.branch2=nn.Sequential(ConvBnSiLu(in_channels,out_channels//2,1,1,0),\n",
        "                                    nn.Conv2d(out_channels//2,out_channels//2,3,2,1,groups=out_channels//2),\n",
        "                                    nn.BatchNorm2d(out_channels//2),\n",
        "                                    ConvBnSiLu(out_channels//2,out_channels//2,1,1,0))\n",
        "        self.channel_shuffle=ChannelShuffle(2)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=torch.cat([self.branch1(x),self.branch2(x)],dim=1)\n",
        "        x=self.channel_shuffle(x) #shuffle two branches\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "oWwgA7n0xeGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeMLP(nn.Module):\n",
        "    '''\n",
        "    naive introduce timestep information to feature maps with mlp and add shortcut\n",
        "    '''\n",
        "    def __init__(self,embedding_dim,hidden_dim,out_dim):\n",
        "        super().__init__()\n",
        "        self.mlp=nn.Sequential(nn.Linear(embedding_dim,hidden_dim),\n",
        "                                nn.SiLU(),\n",
        "                               nn.Linear(hidden_dim,out_dim))\n",
        "        self.act=nn.SiLU()\n",
        "    def forward(self,x,t):\n",
        "        t_emb=self.mlp(t).unsqueeze(-1).unsqueeze(-1)\n",
        "        x=x+t_emb\n",
        "\n",
        "        return self.act(x)"
      ],
      "metadata": {
        "id": "Z5F1qr_TxeEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,time_embedding_dim):\n",
        "        super().__init__()\n",
        "        self.conv0=nn.Sequential(*[ResidualBottleneck(in_channels,in_channels) for i in range(3)],\n",
        "                                    ResidualBottleneck(in_channels,out_channels//2))\n",
        "\n",
        "        self.time_mlp=TimeMLP(embedding_dim=time_embedding_dim,hidden_dim=out_channels,out_dim=out_channels//2)\n",
        "        self.conv1=ResidualDownsample(out_channels//2,out_channels)\n",
        "\n",
        "    def forward(self,x,t=None):\n",
        "        x_shortcut=self.conv0(x)\n",
        "        if t is not None:\n",
        "            x=self.time_mlp(x_shortcut,t)\n",
        "        x=self.conv1(x)\n",
        "\n",
        "        return [x,x_shortcut]"
      ],
      "metadata": {
        "id": "RTi-1LeuxeBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    '''\n",
        "        Upsample process of UNet architecture\n",
        "    '''\n",
        "    def __init__(self,in_channels,out_channels,time_embedding_dim):\n",
        "        super().__init__()\n",
        "        # ---------- **** ---------- #\n",
        "        # YOUR CODE HERE\n",
        "        # Hint: you can refer to the EncoderBlock class\n",
        "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
        "        self.conv0=nn.Sequential(*[ResidualBottleneck(in_channels,in_channels) for i in range(3)],\n",
        "                                    ResidualBottleneck(in_channels,out_channels//2))\n",
        "\n",
        "        self.time_mlp=TimeMLP(embedding_dim=time_embedding_dim,hidden_dim=out_channels,out_dim=out_channels//2)\n",
        "        #self.conv1=ResidualDownsample(out_channels//2,out_channels)\n",
        "\n",
        "        # ---------- **** ---------- #\n",
        "\n",
        "\n",
        "    def forward(self,x,x_shortcut,t=None):\n",
        "\n",
        "        # ---------- **** ---------- #\n",
        "        # YOUR CODE HERE\n",
        "        # Hint: you can refer to the EncoderBlock class and use nn.Upsample\n",
        "\n",
        "        x = self.up(x)\n",
        "        x = torch.cat([x_shortcut, x], dim=1)\n",
        "        if t is not None:\n",
        "            x=self.time_mlp(x_shortcut,t)\n",
        "        x=self.conv0(x)\n",
        "\n",
        "        # ---------- **** ---------- #\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "dgd8qUHTxd_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Unet(nn.Module):\n",
        "\n",
        "    def __init__(self,timesteps,time_embedding_dim,in_channels=3,out_channels=2,base_dim=32,dim_mults=[2,4,8,16]):\n",
        "        super().__init__()\n",
        "        assert isinstance(dim_mults,(list,tuple))\n",
        "        assert base_dim%2==0\n",
        "\n",
        "        channels=self._cal_channels(base_dim,dim_mults)\n",
        "\n",
        "        self.init_conv=ConvBnSiLu(in_channels,base_dim,3,1,1)\n",
        "        self.time_embedding=nn.Embedding(timesteps,time_embedding_dim)\n",
        "\n",
        "        self.encoder_blocks=nn.ModuleList([EncoderBlock(c[0],c[1],time_embedding_dim) for c in channels])\n",
        "        self.decoder_blocks=nn.ModuleList([DecoderBlock(c[1],c[0],time_embedding_dim) for c in channels[::-1]])\n",
        "\n",
        "        self.mid_block=nn.Sequential(*[ResidualBottleneck(channels[-1][1],channels[-1][1]) for i in range(2)],\n",
        "                                        ResidualBottleneck(channels[-1][1],channels[-1][1]//2))\n",
        "\n",
        "        self.final_conv=nn.Conv2d(in_channels=channels[0][0]//2,out_channels=out_channels,kernel_size=1)\n",
        "\n",
        "    def forward(self,x,t=None):\n",
        "        '''\n",
        "            Implement the data flow of the UNet architecture\n",
        "        '''\n",
        "        # ---------- **** ---------- #\n",
        "        # YOUR CODE HERE\n",
        "        t = self.time_embedding\n",
        "\n",
        "        #initial conv\n",
        "        x1 = self.init_conv(x)\n",
        "        #Down\n",
        "        x2 = self.encoder_blocks(x1,t)\n",
        "        x3 = self.encoder_blocks(x2[0],t)\n",
        "        x4 = self.encoder_blocks(x3[0],t)\n",
        "        x5 = self.encoder_blocks(x4[0],t)\n",
        "        #Middle\n",
        "        x6 = self.mid_block(x5[0])\n",
        "        #Up\n",
        "        x = self.decoder_blocks(x6,x5[1],t)\n",
        "        x = self.decoder_blocks(x,x4[1],t)\n",
        "        x = self.decoder_blocks(x,x3[1],t)\n",
        "        x = self.decoder_blocks(x,x2[1],t)\n",
        "        x = self.decoder_blocks(x,x1[1],t)\n",
        "        #final\n",
        "        x = self.final_conv(x)\n",
        "\n",
        "        # ---------- **** ---------- #\n",
        "        return x\n",
        "\n",
        "\n",
        "    def _cal_channels(self,base_dim,dim_mults):\n",
        "        dims=[base_dim*x for x in dim_mults]\n",
        "        dims.insert(0,base_dim)\n",
        "        channels=[]\n",
        "        for i in range(len(dims)-1):\n",
        "            channels.append((dims[i],dims[i+1])) # in_channel, out_channel\n",
        "\n",
        "        return channels\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    x=torch.randn(3,3,224,224)\n",
        "    t=torch.randint(0,1000,(3,))\n",
        "    model=Unet(1000,128)\n",
        "    y=model(x,t)\n",
        "    print(y.shape)"
      ],
      "metadata": {
        "id": "fhoD5IJbxwR0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "76a0657c-8731-42b9-afd8-ed54301770de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Module [ModuleList] is missing the required \"forward\" function",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-36a7bf177d9a>\u001b[0m in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mUnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-36a7bf177d9a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m#Down\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mx3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mx4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \"\"\"\n\u001b[0;32m--> 352\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Module [{type(self).__name__}] is missing the required \"forward\" function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Module [ModuleList] is missing the required \"forward\" function"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "57_g6fPCxd0F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}